{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65881053-57fc-4d83-971c-d725f3348e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import datetime as dt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import wandb\n",
    "from typing import List, Union\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb7f9239-1e8d-49cd-8cec-37e4a3f16a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020011e5-e18c-4c68-88e4-3dde347eb980",
   "metadata": {},
   "source": [
    "##### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74a84040-53e9-4672-a662-d09c25a4939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'fold_num': 5, ## train:valid : 5 :1\n",
    "    'seed': 42,\n",
    "    'img_size': 224,\n",
    "    'model': 'Unet',\n",
    "    'epochs': 200,\n",
    "    'train_bs':32,\n",
    "    'valid_bs':32,\n",
    "    'lr': 1e-4, ## learning rate\n",
    "    'num_workers': 8,\n",
    "    'verbose_step': 1,\n",
    "    'patience' : 5,\n",
    "    'device': 'cuda:0',\n",
    "    'freezing': False,\n",
    "    'model_path': './models'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a405b2a-a008-42fe-a288-9507075c0db5",
   "metadata": {},
   "source": [
    "###### WANDB Init & Model save name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0741d0e3-ba0b-4cbc-a6d8-242979542997",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'satellite'\n",
    "time_now = dt.datetime.now()\n",
    "run_id = time_now.strftime(\"%Y%m%d%H%M\")\n",
    "project_name = category\n",
    "user = 'hojunking'\n",
    "run_name = project_name + '_' + run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e63439-b62a-4c9e-8564-67af0ca05fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c541ebe3-34f0-4053-8de8-5fb4c742087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 디코딩 함수\n",
    "def rle_decode(mask_rle, shape):\n",
    "    if isinstance(mask_rle, int):\n",
    "        s = mask_rle\n",
    "    else:\n",
    "        s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s [0:][::2], s [1:][::2])]\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape [0]*shape [1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img [lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels [1:]!= pixels [:-1])[0] + 1\n",
    "    runs [1::2] -= runs [::2]\n",
    "    return ' '. join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a20bb4-0818-43d2-88b7-9f8d3d662d1c",
   "metadata": {},
   "source": [
    "##### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eaf065c-d46a-4f6f-ba84-ee6cd4a8acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = A.Compose(    [   \n",
    "    A.RandomResizedCrop(p=1, height=CFG['img_size'] ,width=CFG['img_size'], scale=(0.55, 0.75),ratio=(0.90, 1.10)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=True, p=1.0),\n",
    "    A.pytorch.transforms.ToTensorV2()\n",
    "])\n",
    "transform_test = A.Compose([\n",
    "    A.Resize(height = CFG['img_size'], width = CFG['img_size']),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=True, p=1.0),\n",
    "     A.pytorch.transforms.ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19a21490-8152-4b71-8d85-bb97d2343e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../Data/satellite/train.csv')\n",
    "img_path = train_df.iloc[0]['img_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f74ca00d-0335-4b6c-8c69-f786424985df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread('../Data/satellite/'+ img_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9551c8a5-2e9d-489c-8470-f7a3bc41f4e3",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8c9ebff-ce81-458e-9662-9f0cfecf230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, infer=False):\n",
    "        super(SatelliteDataset,self).__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['img_path']\n",
    "        image = cv2.imread('../Data/satellite/'+ img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_rle = self.df.iloc[idx]['mask_rle']\n",
    "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24955461-4520-485a-bd96-f528ee02c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(df, trn_idx, val_idx):\n",
    "    \n",
    "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
    "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
    "        \n",
    "    train_ds = SatelliteDataset(train_, transform=transform_train, infer=False)\n",
    "    valid_ds = SatelliteDataset(valid_, transform=transform_test, infer=False)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG['train_bs'],\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG['num_workers']\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_ds, \n",
    "        batch_size=CFG['valid_bs'],\n",
    "        num_workers=CFG['num_workers'],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ed7a7-c615-435e-9f5b-bedf09a1d898",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "552f735c-6a97-47f8-9ff5-5491c3bbdf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net의 기본 구성 요소인 Double Convolution Block을 정의합니다.\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "# 간단한 U-Net 모델 정의\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)   \n",
    "\n",
    "        x = self.dconv_down4(x)\n",
    "\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)       \n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)   \n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73059032-cb8d-4cd3-8fb5-97a505375f14",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d611093c-9acf-4bda-a2dc-331db93e8b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    # SET MODEL TRAINING MODE\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = None\n",
    "    loss_sum = 0\n",
    "    image_preds_all = []\n",
    "    image_targets_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (imgs, masks) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        masks = masks.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # MODEL PREDICTION\n",
    "        with torch.cuda.amp.autocast():\n",
    "            image_preds = model(imgs)   #output = model(input)\n",
    "            loss = loss_fn(image_preds, masks.unsqueeze(1)) # CRITERION\n",
    "            loss_sum+=loss.detach()\n",
    "            \n",
    "            # BACKPROPAGATION\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "            if running_loss is None:\n",
    "                running_loss = loss.item()\n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01    \n",
    "        \n",
    "            # TQDM VERBOSE_STEP TRACKING\n",
    "            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
    "                pbar.set_description(description)\n",
    "        \n",
    "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [masks.detach().cpu().numpy()]\n",
    "        \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    \n",
    "    trn_loss = loss_sum/len(train_loader)\n",
    "    \n",
    "    return image_preds_all, trn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b7447-0fd4-4636-888e-452b49069a2e",
   "metadata": {},
   "source": [
    "##### Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf021536-7b86-4824-9516-98d8f9503b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    # SET MODEL VALID MODE\n",
    "    model.eval()\n",
    "    \n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    avg_loss = 0\n",
    "    image_preds_all = []\n",
    "    image_targets_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for step, (imgs, masks) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        masks = masks.to(device).float()\n",
    "        \n",
    "        # MODEL PREDICTION\n",
    "        image_preds = model(imgs)\n",
    "        \n",
    "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [masks.detach().cpu().numpy()]\n",
    "        \n",
    "        loss = loss_fn(image_preds, masks.unsqueeze(1))\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        loss_sum += loss.item()*masks.shape[0]\n",
    "        sample_num += masks.shape[0]\n",
    "        \n",
    "        # TQDM\n",
    "        description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
    "        pbar.set_description(description)\n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    val_loss = avg_loss/len(val_loader)\n",
    "    \n",
    "    return image_preds_all, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d07ddf9b-a7ae-42c4-9083-2d8b7867a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, score):\n",
    "        print(f' present score: {score}')\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score >= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            print(f'Best loss from now: {self.best_score :.5f}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8441c8df-b712-440e-8236-c982bc880a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhojunking\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hojun/git/satellite_image_building_area_segmentation/wandb/run-20230723_220344-6tk1d57w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hojunking/satellite/runs/6tk1d57w' target=\"_blank\">noble-jazz-11</a></strong> to <a href='https://wandb.ai/hojunking/satellite' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hojunking/satellite' target=\"_blank\">https://wandb.ai/hojunking/satellite</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hojunking/satellite/runs/6tk1d57w' target=\"_blank\">https://wandb.ai/hojunking/satellite/runs/6tk1d57w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Unet\n",
      "Start with train :5712, valid :1428\n",
      "Fold: 0\n",
      "Epoch 0/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/45 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot select an axis to squeeze out which has size not equal to one",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 75\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# TRAINIG\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# train_preds_all, train_loss = train_one_epoch(epoch, model, loss_tr,\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#                                                             optimizer, train_loader, device, scheduler=scheduler)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# wandb.log({'Train Loss' : train_loss, 'epoch' : epoch})\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# VALIDATION\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 75\u001b[0m     valid_preds_all, valid_loss, valid_dice\u001b[38;5;241m=\u001b[39m \u001b[43mvalid_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid Loss\u001b[39m\u001b[38;5;124m'\u001b[39m : valid_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid Coefficient\u001b[39m\u001b[38;5;124m'\u001b[39m: valid_dice ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m : epoch})\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Train Loss : [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Val Loss : [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Val F1 Score : [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_dice\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 30\u001b[0m, in \u001b[0;36mvalid_one_epoch\u001b[0;34m(epoch, model, loss_fn, val_loader, device, scheduler)\u001b[0m\n\u001b[1;32m     27\u001b[0m gt_masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(masks)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     29\u001b[0m pred_masks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(pred_masks, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m gt_masks \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m pred_masks \u001b[38;5;241m=\u001b[39m (pred_masks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.35\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8) \u001b[38;5;66;03m# Threshold = 0.35\u001b[39;00m\n\u001b[1;32m     33\u001b[0m gt_masks \u001b[38;5;241m=\u001b[39m (gt_masks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.35\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8) \u001b[38;5;66;03m# Threshold = 0.35\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1558\u001b[0m, in \u001b[0;36msqueeze\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m squeeze()\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed_everything(CFG['seed'])\n",
    "    \n",
    "    # WANDB TRACKER INIT\n",
    "    wandb.init(project=project_name, entity=user)\n",
    "    wandb.config.update(CFG)\n",
    "    wandb.run.name = run_name\n",
    "    wandb.define_metric(\"Train Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train-Valid Coefficient\", step_metric=\"epoch\")\n",
    "    \n",
    "    model_dir = CFG['model_path'] + '/{}'.format(run_name)\n",
    "    #train_dir = train.dir.values\n",
    "    best_fold = 0\n",
    "    best_loss = 1\n",
    "    print('Model: {}'.format(CFG['model']))\n",
    "    # MAKE MODEL DIR\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    train_df = pd.read_csv('../Data/satellite/train.csv')\n",
    "    # STRATIFIED K-FOLD DEFINITION\n",
    "    folds = KFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train_df.shape[0]))\n",
    "    \n",
    "    # TEST PROCESS FOLD BREAK\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "    \n",
    "        print(f'Start with train :{len(trn_idx)}, valid :{len(val_idx)}')\n",
    "        print(f'Training start with fold: {fold} epoch: {CFG[\"epochs\"]} \\n')\n",
    "    \n",
    "        # EARLY STOPPING DEFINITION\n",
    "        early_stopping = EarlyStopping(patience=CFG[\"patience\"], verbose=True)\n",
    "    \n",
    "        # DATALOADER DEFINITION\n",
    "        train_loader, val_loader = prepare_dataloader(train_df, trn_idx, val_idx)\n",
    "    \n",
    "        # MODEL & DEVICE DEFINITION \n",
    "        device = torch.device(CFG['device'])\n",
    "        model =UNet()\n",
    "        \n",
    "        # # MODEL FREEZING\n",
    "        # #model.freezing(freeze = CFG['freezing'], trainable_layer = CFG['trainable_layer'])\n",
    "        # if CFG['freezing'] ==True:\n",
    "        #     for name, param in model.named_parameters():\n",
    "        #         if param.requires_grad == True:\n",
    "        #             print(f\"{name}: {param.requires_grad}\")\n",
    "    \n",
    "        model.to(device)\n",
    "        # MODEL DATA PARALLEL\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    \n",
    "        scaler = torch.cuda.amp.GradScaler()   \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=5)\n",
    "    \n",
    "        # CRITERION (LOSS FUNCTION)\n",
    "        loss_tr = torch.nn.BCEWithLogitsLoss()\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "        wandb.watch(model, loss_tr, log='all')\n",
    "        # SAVE ALL RESULTS\n",
    "        valid_loss_list = []\n",
    "        \n",
    "        start = time.time()\n",
    "        print(f'Fold: {fold}')\n",
    "        for epoch in range(CFG['epochs']):\n",
    "            print('Epoch {}/{}'.format(epoch, CFG['epochs'] - 1))\n",
    "    \n",
    "            # TRAINIG\n",
    "            train_preds_all, train_loss = train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, device, scheduler=scheduler)\n",
    "            wandb.log({'Train Loss' : train_loss, 'epoch' : epoch})\n",
    "    \n",
    "            # VALIDATION\n",
    "            with torch.no_grad():\n",
    "                valid_preds_all, valid_loss = valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None)\n",
    "                wandb.log({'Valid Loss' : valid_loss ,'epoch' : epoch})\n",
    "            print(f'Epoch [{epoch}], Train Loss : [{train_loss :.5f}] Val Loss : [{valid_loss :.5f}]')\n",
    "            \n",
    "\n",
    "    \n",
    "            # MODEL SAVE (THE BEST MODEL OF ALL OF FOLD PROCESS)\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                best_epoch = epoch\n",
    "                # SAVE WITH DATAPARARELLEL WRAPPER\n",
    "                #torch.save(model.state_dict(), (model_dir+'/{}.pth').format(CFG['model']))\n",
    "                # SAVE WITHOUT DATAPARARELLEL WRAPPER\n",
    "                torch.save(model.module.state_dict(), (model_dir+'/{}.pth').format(CFG['model']))\n",
    "    \n",
    "            # EARLY STOPPING\n",
    "            stop = early_stopping(valid_loss)\n",
    "            if stop:\n",
    "                print(\"stop called\")   \n",
    "                break\n",
    "    \n",
    "        end = time.time() - start\n",
    "        time_ = str(dt.timedelta(seconds=end)).split(\".\")[0]\n",
    "        print(\"time :\", time_)\n",
    "    \n",
    "        # PRINT BEST F1 SCORE MODEL OF FOLD\n",
    "        best_index = valid_loss_list.index(min(valid_loss_list))\n",
    "        print(f'fold: {fold}, Best Epoch : {best_index}/ {len(valid_loss_list)}')\n",
    "        print(f'Best valid_loss : {valid_loss_list[best_index]:.5f}')\n",
    "        print('-----------------------------------------------------------------------')\n",
    "\n",
    "    # K-FOLD END\n",
    "    if valid_loss_list[best_index] < best_fold:\n",
    "        best_fold = valid_loss_list[best_index]\n",
    "        top_fold = fold\n",
    "    print(f'Best valid_loss: {best_fold} Top fold : {top_fold}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4b916-00cf-4abc-9030-b77c70221a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =pd.read_csv('../Data/satellite/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c0ff4d-d39b-4632-b567-b691412590b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SatelliteDataset(test_df, transform=transform_test, infer=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc16a3-b4fa-479e-8720-4c2dafe88181",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for images in tqdm(test_dataloader):\n",
    "        images = images.float().to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                result.append(-1)\n",
    "            else:\n",
    "                result.append(mask_rle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f47633-971a-4d79-bfe5-c4f5e934b57a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
